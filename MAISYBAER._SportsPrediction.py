# -*- coding: utf-8 -*-
"""FIFA Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-4gMmmZtG18277pfjIaQysAAf1dWHIMZ
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

fifa = pd.read_csv( r"C:\Users\hp\OneDrive\Documents\Ashesi\Year 2_Sem 2\Introduction to AI\Fifa\male_players (legacy).csv",low_memory=False)
fifa

"""1: DATA PREPERATION"""

#Removing spaces

for col in fifa:
    if fifa[col].isnull().mean()>0.2:
        fifa.drop(columns=col,inplace=True)

L=[]
L_less=[]
for i in fifa.columns:
  if((fifa[i].isnull().sum())<(0.4*(fifa.shape[0]))):
    L.append(i)
  else:
    L_less.append(i)

L_less #There is no empty columns

numeric_data=fifa.select_dtypes(include=np.number)
non_numeric=fifa.select_dtypes(include = ['object'])

numeric_data.info()

non_numeric.info()

#Extract data
stats=pd.concat([numeric_data.iloc[:,3:7],numeric_data.iloc[:,17:]],axis=1)
catstats=pd.concat([non_numeric.iloc[:,7:9],non_numeric.iloc[:,12:14]],axis=1)

#Imputing data
from numpy import datetime64, float64, int64
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.impute import SimpleImputer

catPipe = Pipeline([("impute", SimpleImputer(strategy="most_frequent"))])
fullPipe2= ColumnTransformer ([("cat", catPipe, make_column_selector(dtype_include=object))])
piped2 = fullPipe2.fit_transform(catstats)
catstats= pd.DataFrame(data=piped2, columns=fullPipe2.get_feature_names_out())
catstats

quantPipe = Pipeline([ ("impute", SimpleImputer(strategy="median")) ])
fullPipe1= ColumnTransformer ([("quant", quantPipe, make_column_selector (dtype_include=[float64, int64]))])
piped1 = fullPipe1.fit_transform(stats)
stats= pd.DataFrame(data=piped1, columns=fullPipe1.get_feature_names_out())
stats

catstats.columns

#encoding data
from numpy import array
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
label_encoder=LabelEncoder()
integer_encoded=label_encoder.fit_transform(catstats['cat__club_name'])
integer_encoded=integer_encoded.reshape(len(integer_encoded),1)

encodedcat=pd.DataFrame(integer_encoded, columns=['cat__club_name'])

label_encoder=LabelEncoder()
integer_encoded=label_encoder.fit_transform(catstats['cat__club_position'])
integer_encoded=integer_encoded.reshape(len(integer_encoded),1)
encodedcat['cat__club_position']=integer_encoded

label_encoder=LabelEncoder()
integer_encoded=label_encoder.fit_transform(catstats['cat__work_rate'])
integer_encoded=integer_encoded.reshape(len(integer_encoded),1)
encodedcat['cat__work_rate']=integer_encoded

label_encoder=LabelEncoder()
integer_encoded=label_encoder.fit_transform(catstats['cat__body_type'])
integer_encoded=integer_encoded.reshape(len(integer_encoded),1)
encodedcat['cat__body_type']=integer_encoded

encodedcat

data=pd.concat([stats,encodedcat],axis=1).reset_index(drop=True)
data

"""2: CORRELATION"""

corr_matrix=data.corr()
finalcorr=corr_matrix['quant__overall'].sort_values(ascending=False)
print(finalcorr)

#The following features that contribute reletively significantly to the overal score are: quant__movement_reactions, quant__potential,quant__passing,quant__wage_eur,quant__value_eur,quant__dribbling

finaldata=data[['quant__movement_reactions', 'quant__potential','quant__passing','quant__wage_eur','quant__value_eur','quant__dribbling']]
finaldata

"""3:TRAINING AND TESTING"""

#defining variables
y=stats['quant__overall']

X=finaldata

y

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X=scaler.fit_transform(X)
X

#Models

Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
!pip install xgboost
from xgboost import XGBRegressor

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score, auc
import pickle as pkl

rf=RandomForestClassifier(n_estimators=100)
gb=GradientBoostingClassifier(n_estimators=100)
xgb=XGBRegressor(n_estimators=100)

for model in (rf,gb):
 model.fit(Xtrain, Ytrain)
 pkl.dump(model, open('FIFA Assignment.ipynb' + model.__class__.__name__ + '.pkl', 'wb'))
 y_pred = model.predict(Xtest)
 print(model.__class__.__name__, confusion_matrix(Ytest, y_pred), classification_report(Ytest, y_pred))

#Between the two models Random Forest Classifier is more accurate

#Cross Validation

from sklearn.model_selection import KFold, cross_val_score

kfolds=2
for model in (rf,gb,xgb):
    cvscores=cross_val_score(model,X,y,cv=kfolds)
    print(model)
    print('CV Score',cvscores)
    print('Average Score',cvscores.mean())

#XGB Regressor has the best score

"""4:PERFORMANCE"""

for model in (rf,xgb):
    model.fit(Xtrain,Ytrain)
    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score
    y_pred=model.predict(Xtest)
    print(model,f"""Mean Absolute Error={mean_absolute_error(y_pred,Ytest)},
        Mean Squared Error={mean_squared_error(y_pred,Ytest)},
        Root Mean Squared Error={np.sqrt(mean_squared_error(y_pred,Ytest))},
        R2 Score={r2_score(y_pred,Ytest)}""")

#Overall XGB regressor has a better performance

#Tuning model **

from sklearn.model_selection import GridSearchCV, KFold

PARAMETERS_gb ={
"max_depth":[2,5, 6, 12],
"learning_rate":[0.3, 0.1, 0.03],
"n_estimators":[100,500,1000]}

from sklearn.model_selection import GridSearchCV, KFold
model_gs=GridSearchCV(xgb,param_grid=PARAMETERS_gb,cv=5,scoring="accuracy")
model_gs.fit(Xtrain, Ytrain)
pkl.dump(model_gs, open('Ensemble_Classification.ipynb' + model_gs.__class__.__name__ + '.pkl', 'wb'))
y_pred = model_gs.predict(Xtest)
print(model_gs.__class__.__name__, confusion_matrix(Ytest, y_pred), classification_report(Ytest, y_pred))

model_gs.best_params_

model_gs.best_score_

"""5: players_22 TESTING"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor

players22 = pd.read_csv( r"C:\Users\hp\OneDrive\Documents\Ashesi\Year 2_Sem 2\Introduction to AI\Fifa\players_22-1.csv",low_memory=False)
players22

for col in players22 :
    if players22 [col].isnull().mean()>0.2:
        players22 .drop(columns=col,inplace=True)

finalTdata=players22 [['overall','movement_reactions', 'potential','passing','wage_eur','value_eur','dribbling']]
finalTdata

from numpy import datetime64, float64, int64
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.impute import SimpleImputer
quantPipe = Pipeline([ ("impute", SimpleImputer(strategy="median")) ])
fullPipe= ColumnTransformer ([("quant", quantPipe, make_column_selector (dtype_include=[float64, int64]))])
piped= fullPipe.fit_transform(finalTdata)
finalTdata= pd.DataFrame(data=piped, columns=fullPipe.get_feature_names_out())
finalTdata

corr_matrix=finalTdata.corr()
finalcorr=finalTdata['quant__overall'].sort_values(ascending=False)
print(finalcorr)

y=finalTdata['quant__overall']
X=finalTdata[1:]

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X=scaler.fit_transform(X)
X

y

Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)

xgb=XGBRegressor(n_estimators=100)

xgbfit(Xtrain,Ytrain)
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score
y_pred=xgb.predict(Xtest)
print(model,f"""Mean Absolute Error={mean_absolute_error(y_pred,Ytest)},
        Mean Squared Error={mean_squared_error(y_pred,Ytest)},
        Root Mean Squared Error={np.sqrt(mean_squared_error(y_pred,Ytest))},
        R2 Score={r2_score(y_pred,Ytest)}""")

"""6: WEBSITE"""

import requests
!pip install streamlit
import streamlit as st

streamlit run FIFA Assignment.ipynb

st.title ("Fifa Score Prediction")

st.movement_reactions('Pick a number', 0,100)
st.potential('Pick a number', 0,100)
st.passing('Pick a number', 0,100)
st.wage_eur('Pick a number', 0,1000000000)
st.value_eur('Pick a number', 0,1000000000)
st.dribbling('Pick a number', 0,100)

df=pd.Dataframe('movement_reactions':movement_reactions,'potential':potential,'passing':passing,'wage_eur':wage_eur,'value_eur':value_eur,'dribbling':dribbling)

X=0

y=df

Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)

xgb=XGBRegressor(n_estimators=100)

xgbfit(Xtrain,Ytrain)
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score
y_pred=xgb.predict(Xtest)
print(model,f"""Mean Absolute Error={mean_absolute_error(y_pred,Ytest)},
        Mean Squared Error={mean_squared_error(y_pred,Ytest)},
        Root Mean Squared Error={np.sqrt(mean_squared_error(y_pred,Ytest))},
        R2 Score={r2_score(y_pred,Ytest)}""")